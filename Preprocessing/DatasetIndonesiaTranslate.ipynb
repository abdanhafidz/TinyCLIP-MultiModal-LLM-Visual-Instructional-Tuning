{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install aria2"
      ],
      "metadata": {
        "id": "iKHHVI_qnT5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets ctranslate2"
      ],
      "metadata": {
        "id": "R8LcvCRFU2b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt870DMZQdgx"
      },
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "e8D6rRD6wJMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"Lin-Chen/ShareGPT4V\", \"ShareGPT4V\")"
      ],
      "metadata": {
        "id": "uUDnU7vSUtnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "UFrm5TwRVcrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import datasets\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import random\n",
        "import threading\n",
        "\n",
        "def translate_dataset(\n",
        "    dataset_name=None,\n",
        "    dataset=None,\n",
        "    target_language=\"Spanish\",\n",
        "    api_key=None,\n",
        "    site_url=None,\n",
        "    site_name=None,\n",
        "    max_examples=None,\n",
        "    start_idx=0,\n",
        "    checkpoint_file=\"translation_checkpoint.json\",\n",
        "    max_retries=5,\n",
        "    num_threads=4  # New parameter for controlling number of threads\n",
        "):\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Please provide your OpenRouter API key\")\n",
        "\n",
        "    if dataset is None and dataset_name is None:\n",
        "        raise ValueError(\"Please provide either a dataset or dataset_name\")\n",
        "\n",
        "    # Initialize the OpenRouter client\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=api_key,\n",
        "    )\n",
        "\n",
        "    # Extra headers for OpenRouter\n",
        "    extra_headers = {}\n",
        "    if site_url:\n",
        "        extra_headers[\"HTTP-Referer\"] = site_url\n",
        "    if site_name:\n",
        "        extra_headers[\"X-Title\"] = site_name\n",
        "\n",
        "    # Load the dataset if name is provided\n",
        "    if dataset_name:\n",
        "        print(f\"Loading dataset {dataset_name}...\")\n",
        "        dataset = datasets.load_dataset(dataset_name)\n",
        "\n",
        "    if isinstance(dataset, datasets.Dataset):\n",
        "        dataset = datasets.DatasetDict({'train': dataset})\n",
        "\n",
        "    print(f\"Dataset loaded with splits: {', '.join(dataset.keys())}\")\n",
        "\n",
        "    # Thread-safe checkpoint handling\n",
        "    checkpoint_lock = threading.Lock()\n",
        "\n",
        "    def save_checkpoint(split, idx, example, checkpoint_data):\n",
        "        with checkpoint_lock:\n",
        "            if split not in checkpoint_data:\n",
        "                checkpoint_data[split] = {\"current_idx\": 0, \"examples\": []}\n",
        "            checkpoint_data[split][\"examples\"].append(example)\n",
        "            checkpoint_data[split][\"current_idx\"] = max(\n",
        "                checkpoint_data[split][\"current_idx\"],\n",
        "                idx + 1\n",
        "            )\n",
        "            with open(checkpoint_file, 'w') as f:\n",
        "                json.dump(checkpoint_data, f)\n",
        "\n",
        "    def translate_with_retry(content, retries=0, max_wait=60):\n",
        "        \"\"\"Helper function to translate with exponential backoff for retries\"\"\"\n",
        "        prompt = f\"Translate the following text to {target_language}. Preserve any formatting, keep <image> tags unchanged, and maintain the structure of the text, PROVIDE JUST THE TRANSLATION, DO NOT ADD ANYTHING:\\n\\n{content}\"\n",
        "\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                extra_headers=extra_headers,\n",
        "                model=\"meta-llama/llama-4-scout\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "            return completion.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            if retries < max_retries:\n",
        "                wait_time = min(2 ** retries + random.random(), max_wait)\n",
        "                print(f\"API error: {str(e)}. Retrying in {wait_time:.2f} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "                return translate_with_retry(content, retries + 1, max_wait)\n",
        "            else:\n",
        "                print(f\"Max retries exceeded. Error: {str(e)}\")\n",
        "                return content\n",
        "\n",
        "    def process_example(args):\n",
        "        split, idx, example = args\n",
        "        translated_example = copy.deepcopy(example)\n",
        "\n",
        "        if \"conversations\" in example:\n",
        "            translated_conversations = []\n",
        "            for message in example[\"conversations\"]:\n",
        "                role = message[\"from\"]\n",
        "                content = message[\"value\"]\n",
        "                translated_content = translate_with_retry(content)\n",
        "                translated_conversations.append({\n",
        "                    \"from\": role,\n",
        "                    \"value\": translated_content\n",
        "                })\n",
        "            translated_example[\"conversations\"] = translated_conversations\n",
        "        else:\n",
        "            for key, value in example.items():\n",
        "                if isinstance(value, str):\n",
        "                    translated_example[key] = translate_with_retry(value)\n",
        "\n",
        "        return split, idx, translated_example\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint = {}\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        print(f\"Loading checkpoint from {checkpoint_file}\")\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "\n",
        "    translated_datasets = {}\n",
        "\n",
        "    for split in dataset:\n",
        "        print(f\"\\nProcessing {split} split...\")\n",
        "\n",
        "        split_checkpoint = checkpoint.get(split, {})\n",
        "        current_idx = split_checkpoint.get(\"current_idx\", start_idx)\n",
        "        translated_examples = split_checkpoint.get(\"examples\", [])\n",
        "\n",
        "        print(f\"Starting from example {current_idx}\")\n",
        "        if translated_examples:\n",
        "            print(f\"Found {len(translated_examples)} previously translated examples\")\n",
        "\n",
        "        end_idx = len(dataset[split])\n",
        "        if max_examples:\n",
        "            end_idx = min(current_idx + max_examples, end_idx)\n",
        "\n",
        "        # Prepare tasks for parallel processing\n",
        "        tasks = [(split, idx, dataset[split][idx])\n",
        "                for idx in range(current_idx, end_idx)]\n",
        "\n",
        "        # Process examples in parallel\n",
        "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "            futures = [executor.submit(process_example, task) for task in tasks]\n",
        "\n",
        "            for future in tqdm(\n",
        "                concurrent.futures.as_completed(futures),\n",
        "                total=len(tasks),\n",
        "                desc=f\"Translating {split}\"\n",
        "            ):\n",
        "                split, idx, translated_example = future.result()\n",
        "                save_checkpoint(split, idx, translated_example, checkpoint)\n",
        "\n",
        "        # Create dataset from checkpoint\n",
        "        translated_datasets[split] = datasets.Dataset.from_list(\n",
        "            checkpoint[split][\"examples\"]\n",
        "        )\n",
        "\n",
        "    return datasets.DatasetDict(translated_datasets)\n",
        "\n",
        "def load_from_checkpoint(checkpoint_file=\"translation_checkpoint.json\"):\n",
        "    \"\"\"\n",
        "    Load a translated dataset from a checkpoint file.\n",
        "\n",
        "    Parameters:\n",
        "    - checkpoint_file: Path to the checkpoint file\n",
        "\n",
        "    Returns:\n",
        "    - Translated dataset as a DatasetDict\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint_file):\n",
        "        raise FileNotFoundError(f\"Checkpoint file {checkpoint_file} not found\")\n",
        "\n",
        "    print(f\"Loading translations from checkpoint: {checkpoint_file}\")\n",
        "    with open(checkpoint_file, 'r') as f:\n",
        "        checkpoint = json.load(f)\n",
        "\n",
        "    translated_datasets = {}\n",
        "\n",
        "    for split, split_data in checkpoint.items():\n",
        "        examples = split_data.get(\"examples\", [])\n",
        "        print(f\"Found {len(examples)} examples for {split} split\")\n",
        "        translated_datasets[split] = datasets.Dataset.from_list(examples)\n",
        "\n",
        "    return datasets.DatasetDict(translated_datasets)\n",
        "\n",
        "# Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     import argparse\n",
        "\n",
        "#     parser = argparse.ArgumentParser(description=\"Translate the ShareGPT4V dataset\")\n",
        "#     parser.add_argument(\"--api_key\", type=str, required=True, help=\"OpenRouter API key\")\n",
        "#     parser.add_argument(\"--target_language\", type=str, default=\"Spanish\", help=\"Target language for translation\")\n",
        "#     parser.add_argument(\"--site_url\", type=str, help=\"Your site URL for OpenRouter rankings\")\n",
        "#     parser.add_argument(\"--site_name\", type=str, help=\"Your site name for OpenRouter rankings\")\n",
        "#     parser.add_argument(\"--max_examples\", type=int, help=\"Maximum number of examples to translate\")\n",
        "#     parser.add_argument(\"--start_idx\", type=int, default=0, help=\"Index to start from\")\n",
        "#     parser.add_argument(\"--checkpoint_file\", type=str, default=\"translation_checkpoint.json\", help=\"Checkpoint file path\")\n",
        "#     parser.add_argument(\"--output_dir\", type=str, default=\"translated_ShareGPT4V\", help=\"Output directory for translated dataset\")\n",
        "#     parser.add_argument(\"--load_only\", action=\"store_true\", help=\"Only load from checkpoint, don't translate\")\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     if args.load_only:\n",
        "#         # Just load from checkpoint\n",
        "#         translated_dataset = load_from_checkpoint(args.checkpoint_file)\n",
        "#     else:\n",
        "#         # Translate the dataset\n",
        "#         translated_dataset = translate_dataset(\n",
        "#             target_language=args.target_language,\n",
        "#             api_key=args.api_key,\n",
        "#             site_url=args.site_url,\n",
        "#             site_name=args.site_name,\n",
        "#             max_examples=args.max_examples,\n",
        "#             start_idx=args.start_idx,\n",
        "#             checkpoint_file=args.checkpoint_file\n",
        "#         )\n",
        "\n",
        "#     # Save the translated dataset\n",
        "#     print(f\"Saving translated dataset to {args.output_dir}\")\n",
        "#     translated_dataset.save_to_disk(args.output_dir)\n",
        "#     print(\"Done!\")"
      ],
      "metadata": {
        "id": "eKfJoPmAXRfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "\n",
        "# ds = translate_dataset(\n",
        "#     target_language=\"Indonesian\",\n",
        "#     api_key=userdata.get('OPENROUTER_API_KEY'),\n",
        "#     site_url=None,\n",
        "#     site_name=None,\n",
        "#     max_examples=1,\n",
        "#     start_idx=0,\n",
        "#     checkpoint_file=\"translation_checkpoint.json\",\n",
        "# )\n",
        "\n",
        "# ds"
      ],
      "metadata": {
        "id": "ra8CGPD8iiOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ds[\"train\"][\"conversations\"]"
      ],
      "metadata": {
        "id": "nnuIRpfX3IuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/dataset"
      ],
      "metadata": {
        "id": "zfqCmEUsxujp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base path for the project\n",
        "BASE_PATH = \"/content/dataset\""
      ],
      "metadata": {
        "id": "EbTETJhGx2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/dataset/sharegpt4v"
      ],
      "metadata": {
        "id": "NaHd4fQmy9Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!aria2c -x 2 --auto-file-renaming=false https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/resolve/main/share-captioner_coco_lcs_sam_1246k_1107.json?download=true -o dataset/sharegpt4v/share-captioner_coco_lcs_sam_1246k_1107.json\n",
        "!aria2c -x 2 --auto-file-renaming=false https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/resolve/main/sharegpt4v_instruct_gpt4-vision_cap100k.json?download=true -o dataset/sharegpt4v/sharegpt4v_instruct_gpt4-vision_cap100k.json\n",
        "!aria2c -x 2 --auto-file-renaming=false https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/resolve/main/sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json?download=true -o dataset/sharegpt4v/sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json"
      ],
      "metadata": {
        "id": "VLDG5IBGzAIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://files.catbox.moe/wdlcqp.parquet"
      ],
      "metadata": {
        "id": "WJY57yrD3ijV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp = datasets.load_dataset(\"parquet\", data_files=\"wdlcqp.parquet\")"
      ],
      "metadata": {
        "id": "zVFeS9TAIGQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp[\"train\"][\"image_path\"]"
      ],
      "metadata": {
        "id": "cK0Ph8XCIMuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp[\"train\"][0]"
      ],
      "metadata": {
        "id": "5p8Art3WINb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_format(old_entry):\n",
        "    new_entry = {\n",
        "        'id': old_entry['id'],\n",
        "        'image': old_entry['image_path'],\n",
        "        'conversations': [\n",
        "            {\n",
        "                'from': 'human',\n",
        "                'value': old_entry['human_messages'][0]\n",
        "            },\n",
        "            {\n",
        "                'from': 'gpt',\n",
        "                'value': old_entry['assistant_messages'][0]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    return new_entry"
      ],
      "metadata": {
        "id": "0jNY9gD4URYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset = {split: [transform_format(entry) for entry in data]\n",
        "              for split, data in dp.items()}"
      ],
      "metadata": {
        "id": "WLG-h-cHTxfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset"
      ],
      "metadata": {
        "id": "2f0IlkX-URuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "translated_ds = translate_dataset(\n",
        "    dataset=new_dataset,\n",
        "    target_language=\"Indonesian\",\n",
        "    api_key=userdata.get('OPENROUTER_API_KEY'),\n",
        "    site_url=None,\n",
        "    site_name=None,\n",
        "    start_idx=0,\n",
        "    checkpoint_file=\"translation_checkpoint.json\",\n",
        "    num_threads=128\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x87RA4fUS1v",
        "outputId": "2adad735-c740-4e18-acfb-e28254edb0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with splits: train\n",
            "Loading checkpoint from translation_checkpoint.json\n",
            "\n",
            "Processing train split...\n",
            "Starting from example 20000\n",
            "Found 20000 previously translated examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating train: 0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_ds[\"train\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FqoBLWYUdD4",
        "outputId": "58c46714-13a0-4095-f838-54c14e93bbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'image', 'conversations'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -F \"reqtype=fileupload\" -F \"fileToUpload=@translation_checkpoint.json\" https://catbox.moe/user/api.php"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmX745uQU8sN",
        "outputId": "ca2c137a-9fc9-4f05-d4a9-f756cb2918b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://files.catbox.moe/lzi8zt.json"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_ds[\"train\"][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFd6uA-mJ2w4",
        "outputId": "bf4e72fb-cde1-4410-fa6a-1f91cdac440b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '000000183360',\n",
              " 'image': 'coco/train2017/000000183360.jpg',\n",
              " 'conversations': [{'from': 'human',\n",
              "   'value': '<image>\\nApa yang digambarkan dalam foto ini?'},\n",
              "  {'from': 'gpt',\n",
              "   'value': 'Gambar tersebut menangkap sebuah momen, menampilkan sebuah lokomotif uap hitam yang megah dengan balok penyangga merah cerah. Lokomotif, dengan bangga menampilkan nomor 30587 dalam warna putih, diposisikan di atas jalur kereta api. Ini adalah sebuah adegan aktif, dengan lokomotif mengeluarkan semburan uap dari cerobong asapnya, menunjukkan bahwa lokomotif itu sedang bersiap untuk berangkat atau baru saja tiba.\\n\\nLokomotif menghadap ke sisi kanan gambar, seolah-olah siap untuk memulai perjalanan ke tempat yang tidak diketahui. Latar belakang menyediakan kontras yang tenang dengan lokomotif industri, menampilkan area berhutan yang lebat dengan pohon-pohon dan semak-semak. Kehijauan hutan dan warna hitam dan merah lokomotif menciptakan kontras yang mencolok, menambahkan kedalaman dan minat pada adegan. Seluruh gambar melukiskan gambaran dari sebuah era yang telah berlalu, di mana lokomotif uap adalah jantung dari transportasi dan eksplorasi.'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_ds[\"train\"].to_parquet(\"datasettt.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "851017afbc774fed97ca5fca8a9ed551",
            "a0db4d4db592428a8735a0f83d1c5803",
            "28212399c8184491ab9a7dc542f7929e",
            "ab7a2a34340a43bb9a957318abb986b4",
            "bd2d1e399b4745a0b0f6056c2365b129",
            "dbb10038df124ad38dbbfd324ef0d61c",
            "b88ac5d6b2c64ff08adf59b007806598",
            "dc1502dc97b34701a751a6c1eb726f0d",
            "224be200779a412fba1129337e8b67fa",
            "b521cead8bfd4ba3b553eee64b728795",
            "4b8494b739a1497dbbd3de409817b0c4"
          ]
        },
        "id": "52rdDcPCXDWJ",
        "outputId": "f1008434-5b6d-402d-c97a-52de01788c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "851017afbc774fed97ca5fca8a9ed551"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22719464"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -F \"reqtype=fileupload\" -F \"fileToUpload=@datasettt.parquet\" https://catbox.moe/user/api.php"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AssACE4pZhqA",
        "outputId": "1b7b2a9a-ea5e-4a81-cd00-0050cb70d71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://files.catbox.moe/hfawow.parquet"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLMzACrDmy2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}